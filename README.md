# financial_assistant_bot
ChatBot for financial news

This is ğ—µğ—¼ğ˜„ you can ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² to populate a ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• to do ğ—¥ğ—”ğ—š for a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ powered by ğ—Ÿğ—Ÿğ— ğ˜€.

ğŸ All the following steps are wrapped in Bytewax functions and connected in a single streaming pipeline (aka Bytewax flow) â†“

ğ—˜ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—»ğ—²ğ˜„ğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğ—”ğ—¹ğ—½ğ—®ğ—°ğ—®

You need 2 types of inputs:

1. A WebSocket API to listen to financial news in real time - used to listen 24/7 for new data and ingest it as soon as it is available.

2. A RESTful API to ingest historical data in batch mode. When you deploy a fresh vector DB, you use it to populate it with older data.

You wrap the ingested HTML document and its metadata in a `pydantic` NewsArticle model to validate its schema.

Regardless of the input type, the ingested data is the same. Thus, the following steps are the same for both data inputs â†“

ğ—£ğ—®ğ—¿ğ˜€ğ—² ğ˜ğ—µğ—² ğ—›ğ—§ğ— ğ—Ÿ ğ—°ğ—¼ğ—»ğ˜ğ—²ğ—»ğ˜

As the ingested financial news is in HTML, you must extract the text from particular HTML tags.

`unstructured` makes it as easy as calling `partition_html(document)`, which will recursively return the text within all essential HTML tags.

The parsed NewsArticle model is mapped into another `pydantic` model to validate its new schema:
- the headline
- summary
- full content.

ğ—–ğ—¹ğ—²ğ—®ğ—» ğ˜ğ—µğ—² ğ˜ğ—²ğ˜…ğ˜

Now we have a bunch of text that has to be cleaned. Again, `unstructured` makes things easy. Calling a few functions we clean:
- the dashes & bullets
- extra whitespace & trailing punctuation
- non ascii chars
- invalid quotes

Finally, we standardize everything to lowercase.

ğ—–ğ—µğ˜‚ğ—»ğ—¸ ğ˜ğ—µğ—² ğ˜ğ—²ğ˜…ğ˜

As the text can exceed the context window of the embedding model, we have to chunk it.

Yet again, `unstructured` provides a valuable function that splits the text based on the tokenized text and expected input length of the embedding model.

This strategy is naive, as it doesn't consider the text's structure, such as chapters, paragraphs, etc. As the news is short, this is not an issue, but LangChain provides a `RecursiveCharacterTextSplitter` class that does that if required.

ğ—˜ğ—ºğ—¯ğ—²ğ—± ğ˜ğ—µğ—² ğ—°ğ—µğ˜‚ğ—»ğ—¸ğ˜€

You pass all the chunks through an encoder-only model.

We have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model that can run on a CPU and outputs a 384 embedding.

But based on the size and complexity of your data, you might need more complex and bigger models.

ğ—Ÿğ—¼ğ—®ğ—± ğ˜ğ—µğ—² ğ—±ğ—®ğ˜ğ—® ğ—¶ğ—» ğ˜ğ—µğ—² ğ—¤ğ—±ğ—¿ğ—®ğ—»ğ˜ ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—•

Finally, you insert the embedded chunks and their metadata into the Qdrant vector DB.

The metadata contains the embedded text, the source_url and the publish date.